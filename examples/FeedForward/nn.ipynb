{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Overview**\n",
    "\n",
    "Input Layer -> Hidden Layers -> Output Layer\n",
    "Tune weights and biases to achieve desired results (input layer)\n",
    "note how each neuron only has one bias regardless of input\n",
    "input can be an input value from a layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "\n",
    "output = inputs[0] * weights[0]+ inputs[1] * weights[1] + inputs[2] * weights[2] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "output = inputs[0] * weights[0]+ inputs[1] * weights[1] + inputs[2] * weights[2] + inputs[3] * weights[3] + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 4.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 2.5\n",
    "\n",
    "output = [inputs[0] * weights1[0]+ inputs[1] * weights1[1] + inputs[2] * weights1[2] + inputs[3] * weights1[3] + bias1,\n",
    "          inputs[0] * weights2[0]+ inputs[1] * weights2[1] + inputs[2] * weights2[2] + inputs[3] * weights2[3] + bias2,\n",
    "          inputs[0] * weights3[0]+ inputs[1] * weights3[1] + inputs[2] * weights3[2] + inputs[3] * weights3[3] + bias3]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 4.385]\n"
     ]
    }
   ],
   "source": [
    "# simplified code for input layer\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 2.5]\n",
    "\n",
    "layer_outputs = []\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectors or Matrix or Tensor**\n",
    "\n",
    "list = [1, 5, 6, 2] 1d array or vector with shape (4,1) \n",
    "\n",
    "list = [\n",
    "    [1, 5, 6, 2],   2d array or vector with shape (4,2)\n",
    "    [5, 7, 1, 4]\n",
    "]\n",
    "\n",
    "to avoid shape errors in pandas/numpy/tensorflow: ensure that shapes are HOMOLOGOUS\n",
    " \n",
    "In the context of deep learning, a tensor is an object that can be represented as an array, not just the array itself.\n",
    "\n",
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "\n",
    "dot_product = a[0] * b[0] + a[1] * b[1] + a[2] * b[2]\n",
    "\n",
    "Note that dot product of two vectors results in a single scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "# dot product sample \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "output = np.dot(inputs, weights) + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  4.385]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = [1, 2, 3, 2.5] # 4 features\n",
    "weights = [[0.2, 0.8, -0.5, 1.0], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 2.5]\n",
    "\n",
    "\n",
    "output = np.dot(weights, inputs) + biases\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Size**\n",
    "\n",
    "ne sample at a time, makes it difficult for the neuron to fit the line\n",
    "\n",
    "Why not show all the samples to make it easier for the neuron to fit?\n",
    "It leads to overfitting (hurts generalization and hurts out of sample data) 32 is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   4.385]\n",
      " [ 8.9   -1.81   2.2  ]\n",
      " [ 1.41   1.051  2.026]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5], # this one is (3,4)\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    " \n",
    "weights = [[0.2, 0.8, -0.5, 1.0], # this one is (3,4) -- shape error if not transposed to (4,3)\n",
    "            [0.5, -0.91, 0.26, -0.5],\n",
    "            [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 2.5] # notice how no biases are added since no neurons are added\n",
    "\n",
    "'''\n",
    "reminder: index 1 of first element needs to match index 0 of second element e.g (3,4) amd (4,3)\n",
    "\n",
    "error if no transpose of weights matrix\n",
    "ValueError: shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "output = np.dot(inputs, np.array(weights).T) + biases   # transposed here\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    " \n",
    "weights = [[0.2, 0.8, -0.5, 1.0], \n",
    "            [0.5, -0.91, 0.26, -0.5],\n",
    "            [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5], \n",
    "            [-0.5, 0.12, -0.33],\n",
    "            [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases = [2, 3, 0.5] \n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "# remember Neil, n_inputs is feature numbers (4) and neurons refers to any number you want\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons): # randomly generate weights and bias based on shape of input \n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons)) # idk why i need 2 parentheses here plz figure it out\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases \n",
    "\n",
    "'''\n",
    "Remember the shape shits we talked about earlier, the neuron # for layer 1\n",
    "must be the same as the input # for layer 2\n",
    "'''\n",
    "\n",
    "\n",
    "layer1 = Layer_Dense(4, 5) \n",
    "layer2 = Layer_Dense(5, 2)\n",
    "\n",
    "layer1.forward(X)\n",
    "layer2.forward(layer1.outputs)\n",
    "print(layer1.outputs)\n",
    "print(layer2.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Functions**\n",
    "\n",
    "Every neron in hidden layer has an activation function. Applying the activation function applies after doing dot product.\n",
    "\n",
    "1. Step Function (Linear) - if x > 0: then x = 1. If x < 0: x = 0\n",
    "2. Sigmoid Function - It has a granular output, and ensures better backpropagagtion and loss calculation. \n",
    "3. RELU - If x > 0: then x == x. If x < 0: x == 0. (used due to speed, but has vanishing gradient problem) - offset activation point by tweaking bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    elif i <= 0:\n",
    "        output.append(0)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Activation**\n",
    "\n",
    "Create a probability distribution for the neural network.\n",
    "It accounts for negative values without losing meaning - as opposed to using abolute values or exponents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "E = math.e # 2.71828\n",
    "\n",
    "exp_values = []\n",
    "\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E**output)\n",
    "\n",
    "print(exp_values)\n",
    "\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "\n",
    "for value in exp_values:\n",
    "    norm_values.append(value/norm_base)\n",
    "\n",
    "print(norm_values)\n",
    "print(sum(norm_values)) # add up to 1 or 0.999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999 because of floating point error\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n",
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_outputs = [[4.8, 1.21, 2.385],\n",
    "                 [8.9, -1.81, 0.2],\n",
    "                 [1.41, 1.051, 0.026],]\n",
    "\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print(np.sum(layer_outputs, axis = 1, keepdims = True))\n",
    "\n",
    "norm_values =  exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "print(norm_values)\n",
    "\n",
    "#norm_values = exp_values/np.sum(exp_values)\n",
    "#print(norm_values)\n",
    "#print(sum(norm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function**\n",
    "\n",
    "1. For Regression : Mean Absolute Error or Mean Squared Error is Common\n",
    "2. For Classification: Categorical Cross Entropy\n",
    "\n",
    "solving for x\n",
    "\n",
    "e ** x  = b\n",
    "\n",
    "in machine learning, log is based on e (euler's number 1.7)\n",
    "\n",
    "Classes: 3\n",
    "Label: 0\n",
    "One Hot: [1, 0, 0]\n",
    "Prediction: [0.7, 0.1, 0,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.199999999999999\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "b = 5.2\n",
    "np.log(b)\n",
    "print(math.e ** 1.6486586255873816)\n",
    "\n",
    "# Categorical Cross Entropy (negative log)\n",
    "\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "target_output = [1, 0, 0]\n",
    "target_class = 0\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "         math.log(softmax_output[1])*target_output[1] +\n",
    "         math.log(softmax_output[2])*target_output[2])\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# -(math.log(0.7)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
